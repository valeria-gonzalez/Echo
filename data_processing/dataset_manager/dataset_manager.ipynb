{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from text_difficulty.text_difficulty import TextDifficultyEvaluator\n",
    "from texts_by_category.text_category_evaluator import TextCategoryEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e25028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager:\n",
    "    def __init__(self):\n",
    "        self.difficulty_evaluator = TextDifficultyEvaluator()\n",
    "        self.category_evaluator = TextCategoryEvaluator()\n",
    "        self.filepath_dataset_texts = \"datasets/group_1_texts.jsonl\"\n",
    "        self.destination_path = \"datasets/processed_datasets/\"\n",
    "        self.filepath_dataset_sentences = \"datasets/tatoeba_sentences.jsonl\"\n",
    "        self.filepath_dataset_words = \"datasets/words-de02e1507605431abd5d829d7e868af5.jsonl\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb900df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(self) -> None:\n",
    "    texts = []\n",
    "    with open(self.filepath_dataset_texts, \"r\",\n",
    "        encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        for item in data:\n",
    "            key = item.get(\"chapter_id\", \"\")\n",
    "            transcript = item.get(\"transcript\", \"\")\n",
    "            text = transcript.get(\"full_text\", \"\")\n",
    "            formatted_text = text.lower().capitalize()\n",
    "            transcript[\"full_text\"] = formatted_text\n",
    "            item[\"difficult\"] = self.difficulty_evaluator.text_difficulty(text)\n",
    "            categories = self.category_evaluator.classify_words(text)\n",
    "            three_keys = list(categories.keys())[:3]\n",
    "            item[\"categories\"] = three_keys\n",
    "            print(f\"Text difficulty for {key}: {item['difficult']}\")\n",
    "            \n",
    "            texts.append(item)\n",
    "            \n",
    "        os.makedirs(self.destination_path, exist_ok=True)\n",
    "        json_object = json.dumps(texts)\n",
    "        with open(os.path.join(self.destination_path, \n",
    "        \"texts_processed.jsonl\"), \n",
    "        \"w\", encoding=\"utf-8\") as out_file:\n",
    "            out_file.write(json_object + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc92b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetManager.process_texts = process_texts\n",
    "\n",
    "obj = DatasetManager()\n",
    "obj.process_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01171a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(self) -> None:\n",
    "    sentences = []\n",
    "    with open(self.filepath_dataset_sentences, \"r\",\n",
    "        encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        for item in data:\n",
    "            text = item.get(\"eng_sen\", \"\")\n",
    "            item[\"difficult\"] = self.difficulty_evaluator.sentence_difficulty(text)\n",
    "            categories = self.category_evaluator.classify_words(text)\n",
    "            three_keys = list(categories.keys())[:3]\n",
    "            item[\"categories\"] = three_keys\n",
    "            \n",
    "            sentences.append(item)\n",
    "            \n",
    "        os.makedirs(self.destination_path, exist_ok=True)\n",
    "        json_object = json.dumps(sentences)\n",
    "        with open(os.path.join(self.destination_path, \n",
    "        \"sentences_processed.jsonl\"), \n",
    "        \"w\", encoding=\"utf-8\") as out_file:\n",
    "            out_file.write(json_object + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetManager.process_sentences = process_sentences\n",
    "obj.process_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(self) -> None:\n",
    "    with open(self.filepath_dataset_words, \"r\",\n",
    "        encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            word_entry = json.loads(line)\n",
    "            for word, word_data in word_entry.items():\n",
    "                word_data[\"difficult\"] = self.difficulty_evaluator.word_difficulty(word)\n",
    "                all_texts = []\n",
    "                text = \" \"\n",
    "                definitions = word_data.get(\"definitions\", [])\n",
    "                definition_texts = definitions[0].get(\"definitions\", [])\n",
    "                all_texts.extend(definition_texts)\n",
    "                text = \" \".join(all_texts)\n",
    "                categories = self.category_evaluator.classify_words(text)\n",
    "                three_keys = list(categories.keys())[:3]\n",
    "                word_data[\"categories\"] = three_keys\n",
    "                \n",
    "        os.makedirs(self.destination_path, exist_ok=True)\n",
    "        json_object = json.dumps(word_entry)\n",
    "        with open(os.path.join(self.destination_path, \n",
    "        \"words_processed.jsonl\"), \n",
    "        \"w\", encoding=\"utf-8\") as out_file:\n",
    "            out_file.write(json_object + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetManager.process_words = process_words\n",
    "obj = DatasetManager()\n",
    "obj.process_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_datasets(self) -> None:\n",
    "    process_texts()\n",
    "    process_sentences()\n",
    "    process_words()\n",
    "    print(\"All datasets have been processed and saved successfully.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
